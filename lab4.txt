# user options
st.sidebar.header("LLM Options")
llm = st.sidebar.radio("Choose LLM vendor", ("OpenAI", "Claude"))
advanced_model = st.sidebar.checkbox("Use advanced model")

if llm == "OpenAI":
    model = "gpt-4o-mini"
    if advanced_model:
        model = "gpt-4o"
else:  # Claude
    model = "claude-3-haiku-20240307"
    if advanced_model:
        st.sidebar.write("No premium model available for Claude/anthropic")

# URL inputs
st.sidebar.header("URL input")
url1 = st.sidebar.text_input("Input first url")
url2 = st.sidebar.text_input("Input second url")

# dynamic system prompt
system_prompt = ( "You are a helpful assistant. Always explain things in a simple way a 10-year-old can understand." ) 

if url1: 
    system_prompt += f"\n\nThe user has provided URL 1: {url1}" 
if url2: 
    system_prompt += f"\n\nThe user has provided URL 2: {url2}"

# chat history initialization
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "system", "content": 
            system_prompt},
        {"role": "assistant", "content": "How can I help you?"}
    ]
else:
    st.session_state["messages"][0]["content"] = system_prompt

if "more_info" not in st.session_state:
    st.session_state.more_info = False

# summary initialization
if "summary" not in st.session_state: 
    st.session_state.summary = ""

# display chat history BEFORE input
for msg in st.session_state.messages:
    if msg["role"] != "system":  # Don't display system messages
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

# chat input
if prompt := st.chat_input("What is up?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    if st.session_state.more_info:
        lower = prompt.lower().strip()
        if lower == "yes":
            system_msg = st.session_state.messages[0]
            conversation = st.session_state.messages[1:]
            
            # Token buffer logic
            max_tokens = 2000
            system_tokens = len(system_msg["content"].split())
            buffer = []
            tokens_count = system_tokens
            for msg in reversed(conversation):
                msg_tokens = len(msg["content"].split())
                if tokens_count + msg_tokens > max_tokens:
                    break
                buffer.insert(0, msg)
                tokens_count += msg_tokens
            
            messages = [system_msg] + buffer
            
            # stream based on chosen llm
            if llm == "OpenAI":
                client = st.session_state.openai_client
                stream = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    stream=True
                )
                with st.chat_message("assistant"):
                    more_info = st.write_stream(stream)
                    
            else:  # Claude
                client = st.session_state.claude_client
                system_content = system_msg["content"]
                claude_messages = [msg for msg in messages if msg["role"] != "system"]
                
                with st.chat_message("assistant"):
                    more_info = ""
                    message_placeholder = st.empty()
                    
                    with client.messages.stream(
                        model=model,
                        system=system_content,
                        messages=claude_messages,
                        max_tokens=1000
                    ) as stream:
                        for text in stream.text_stream:
                            more_info += text
                            message_placeholder.markdown(more_info + "▌")
                        message_placeholder.markdown(more_info)
            
            more_info_answer = more_info + "\n\nDo you want more info?"
            st.session_state.messages.append(
                {"role": "assistant", "content": more_info_answer}
            )
        elif lower == "no":
            reply = "What else can I help you with?"
            with st.chat_message("assistant"):
                st.markdown(reply)
            st.session_state.messages.append({"role": "assistant", "content": reply})
            st.session_state.more_info = False
        else:
            reply = "Please reply with Yes or No."
            with st.chat_message("assistant"):
                st.markdown(reply)
            st.session_state.messages.append({"role": "assistant", "content": reply})
    else:
        system_msg = st.session_state.messages[0]
        conversation = st.session_state.messages[1:]
        
        # Token buffer logic
        max_tokens = 2000
        system_tokens = len(system_msg["content"].split())
        buffer = []
        tokens_count = system_tokens
        for msg in reversed(conversation):
            msg_tokens = len(msg["content"].split())
            if tokens_count + msg_tokens > max_tokens:
                break
            buffer.insert(0, msg)
            tokens_count += msg_tokens
        
        messages = [system_msg] + buffer
        
        # stream based on chosen llm
        if llm == "OpenAI":
            client = st.session_state.openai_client
            stream = client.chat.completions.create(
                model=model,
                messages=messages,
                stream=True
            )
            with st.chat_message("assistant"):
                response = st.write_stream(stream)
                
        else:  # Claude
            client = st.session_state.claude_client
            system_content = system_msg["content"]
            claude_messages = [msg for msg in messages if msg["role"] != "system"]
            
            with st.chat_message("assistant"):
                response = ""
                message_placeholder = st.empty()
                
                with client.messages.stream(
                    model=model,
                    system=system_content,
                    messages=claude_messages,
                    max_tokens=1000
                ) as stream:
                    for text in stream.text_stream:
                        response += text
                        message_placeholder.markdown(response + "▌")
                    message_placeholder.markdown(response)
        
        final_response = response + "\n\nDo you want more info?"
        st.session_state.messages.append(
            {"role": "assistant", "content": final_response}
        )
        st.session_state.more_info = True

        # generate summary after 6 messages
        if len(st.session_state.messages) > 7:

            # build conversation
            convo_text = ""
            for m in st.session_state.messages[1:]:
                if m["role"] != "system":
                    convo_text += f"{m['role']}: {m['content']}\n"

            # build summary prompt
            summary_prompt = [
                {"role": "system", "content": "Summarize the following conversation in 3–4 sentences."},
                {"role": "user", "content": convo_text}
            ]

            # generate summary with chosen llm
            if llm == "OpenAI":
                client = st.session_state.openai_client
                result = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=summary_prompt
                )
                summary_text = result.choices[0].message.content

            else:  
                client = st.session_state.claude_client
                result = client.messages.create(
                    model="claude-3-haiku-20240307",
                    max_tokens=1000,
                    system="Summarize the following conversation in 3–4 sentences.",
                    messages=[{"role": "user", "content": convo_text}]
                )
                summary_text = result.content[0].text

            # display summary
            with st.chat_message("assistant"):
                st.markdown(f"Conversation summary so far:\n\n{summary_text}")

            # save summary
            st.session_state.messages = [
                st.session_state.messages[0],  # keep system
                {"role": "assistant", "content": f"Summary so far:\n\n{summary_text}"}
            ]

            # store summary for system prompt memory
            st.session_state.summary = summary_text
